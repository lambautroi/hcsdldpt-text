import os
from pathlib import Path
from TrichRutDacTrung import extract_features_from_file
from LuuTruDacTrung import ClusterUseKmeans, LuuDanhSachDacTrungVaNhom

DATA_DIR = "data"
OUTPUT_FILE = "metadata/data.json"

def main():
    print("ğŸš€ Báº¯t Ä‘áº§u trÃ­ch rÃºt Ä‘áº·c trÆ°ng tá»« thÆ° má»¥c 'data/'...\n")
    
    all_files = []
    list_features = []

    for filename in os.listdir(DATA_DIR):
        filepath = os.path.join(DATA_DIR, filename)
        if filename.endswith(".txt") or filename.endswith(".pdf"):
            print(f"ğŸ“„ Äang xá»­ lÃ½: {filename}")
            try:
                features = extract_features_from_file(filepath)
                list_features.append(features)
                all_files.append(filename)  # chá»‰ láº¥y tÃªn file
            except Exception as e:
                print(f"âŒ Lá»—i khi xá»­ lÃ½ {filename}: {e}")

    print(f"\nâœ… ÄÃ£ trÃ­ch rÃºt Ä‘áº·c trÆ°ng cho {len(list_features)} vÄƒn báº£n.")

    if not list_features:
        print("âš ï¸ KhÃ´ng cÃ³ Ä‘áº·c trÆ°ng nÃ o Ä‘Æ°á»£c trÃ­ch rÃºt. ThoÃ¡t.")
        return

    # Chuáº©n hÃ³a kÃ­ch thÆ°á»›c náº¿u cáº§n (cháº¯c Äƒn)
    import numpy as np
    features_array = np.array(list_features)
    if len(set(len(f) for f in list_features)) > 1:
        print("âš ï¸ CÃ¡c Ä‘áº·c trÆ°ng cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau. Äang chuáº©n hÃ³a láº¡i...")
        from sklearn.preprocessing import StandardScaler
        features_array = StandardScaler().fit_transform(features_array)
    print(f"âœ… Äáº·c trÆ°ng sau chuáº©n hÃ³a, shape: {features_array.shape}")

    # Clustering : PhÃ¢n cá»¥m
    labels = ClusterUseKmeans(features_array)

    # LÆ°u Ä‘áº·c trÆ°ng + nhÃ£n cá»¥m + link
    LuuDanhSachDacTrungVaNhom(features_array, labels, OUTPUT_FILE, file_paths=all_files)

if __name__ == "__main__":
    main()
